# NOTE: This image contains hacks, is subject to change and will be unified with the main image soon

ARG SGLANG_VERSION=latest
ARG SGLANG_IMAGE_TAG
FROM lmsysorg/sglang:${SGLANG_IMAGE_TAG:-${SGLANG_VERSION}} AS sglang

# we need to write this again after from
ARG SGLANG_VERSION
ARG MEGATRON_COMMIT=core_v0.14.0

ARG ENABLE_BLACKWELL_BUILD=0
ARG ENABLE_CUDA_13=0
# Can be 0,1,2, larger value means resource is more limited
# 0: Large machine
# 1: Tested on 64 CPU, 432GB RAM
# 2: Tested on 32 CPU, 64GB RAM
ARG ENABLE_TIGHT_BUILD_RES=0

RUN apt update
RUN apt install -y nvtop rsync numactl

# TODO: change to pip install sglang-router after it has a new release
RUN pip install sglang-router --force-reinstall
RUN pip install git+https://github.com/fzyzcjy/torch_memory_saver.git --no-cache-dir --force-reinstall
RUN pip install ray[default]
RUN pip install httpx[http2] wandb pylatexenc blobfile accelerate "mcp[cli]"

# mbridge
RUN pip install git+https://github.com/ISEEKYAN/mbridge.git --no-deps

RUN if [ "${ENABLE_BLACKWELL_BUILD}" = "1" ]; then \
          export TLIST="8.0;8.9;9.0;9.0a;10.0"; \
        else \
          export TLIST="8.0;8.9;9.0;9.0a"; \
        fi && \
    TORCH_CUDA_ARCH_LIST="$TLIST" pip install git+https://github.com/fanshiqing/grouped_gemm@v1.1.4 --no-build-isolation

# apex
RUN if [ "$ENABLE_TIGHT_BUILD_RES" = "2" ]; then \
          export NVCC_APPEND_FLAGS="--threads 1"; \
        else \
          export NVCC_APPEND_FLAGS="--threads 4"; \
        fi && \
  pip -v install --disable-pip-version-check --no-cache-dir \
  --no-build-isolation \
  --config-settings "--build-option=--cpp_ext --cuda_ext --parallel 8" git+https://github.com/NVIDIA/apex.git

# transformer engine, we install with --no-deps to avoid installing torch and torch-extensions
RUN pip install pybind11

# flash attn
# the newest version megatron supports is v2.7.4
RUN if [ "$ENABLE_TIGHT_BUILD_RES" = "2" ]; then \
      export MAX_JOBS=4; \
    elif [ "$ENABLE_TIGHT_BUILD_RES" = "1" ]; then \
      export MAX_JOBS=32; \
    else \
      export MAX_JOBS=64; \
    fi && \
    pip -v install flash-attn==2.7.4.post1 --no-build-isolation

RUN pip install flash-linear-attention

# TE does not have wheel on cuda 13 yet, thus need to install from source
RUN if [ "${ENABLE_CUDA_13}" = "1" ]; then \
          pip -v install --no-build-isolation git+https://github.com/NVIDIA/TransformerEngine.git@stable; \
        else \
          pip -v install --no-build-isolation "transformer_engine[pytorch]"; \
        fi

RUN git clone https://github.com/Dao-AILab/flash-attention.git && \
    cd flash-attention/ && git submodule update --init && cd hopper/ && python setup.py install && \
    export python_path=`python -c "import site; print(site.getsitepackages()[0])"` && \
    mkdir -p $python_path/flash_attn_3 && \
    cp flash_attn_interface.py $python_path/flash_attn_3/flash_attn_interface.py

WORKDIR /root/
RUN git clone https://github.com/NVIDIA/Megatron-LM.git --recursive && \
    cd Megatron-LM && git checkout ${MEGATRON_COMMIT} && \
    pip install -e .

# sandwitch norm for GLM models
COPY patch/${SGLANG_VERSION}/megatron.patch /root/Megatron-LM/
RUN cd Megatron-LM && \
    git update-index --refresh && \
    git apply megatron.patch --3way && \
    if grep -R -n '^<<<<<<< ' .; then \
      echo "Patch failed to apply cleanly. Please resolve conflicts." && \
      exit 1; \
    fi && \
    rm megatron.patch

# sglang patch
COPY patch/${SGLANG_VERSION}/sglang.patch /sgl-workspace/sglang/
RUN cd /sgl-workspace/sglang && \
  git update-index --refresh && \
  git apply sglang.patch && \
  if grep -R -n '^<<<<<<< ' .; then \
    echo "Patch failed to apply cleanly. Please resolve conflicts." && \
    exit 1; \
  fi && \
  rm sglang.patch

RUN rm /root/.tmux.conf

# This patch from masahi will be included in later Triton releases
RUN (cd /root && git clone -b feat/v350_plus_8045 https://github.com/fzyzcjy/triton.git && cd triton && pip install -r python/requirements.txt && pip install --verbose -e .)

# This patch is merged into main, but we are using stable version, thus still need it
RUN curl -L https://github.com/NVIDIA/TransformerEngine/pull/2286.patch -o /root/te2286.patch && (cd /usr/local/lib/python3.12/dist-packages/transformer_engine && (patch -p2 < /root/te2286.patch))

# temporarily hack fix issue in GB300 base image
RUN SGL_KERNEL_VERSION=0.3.16.post4 && \
    python3 -m pip install https://github.com/sgl-project/whl/releases/download/v${SGL_KERNEL_VERSION}/sgl_kernel-${SGL_KERNEL_VERSION}+cu130-cp310-abi3-manylinux2014_$(uname -m).whl --force-reinstall --no-deps

# temporary hack
# NOTE: must patch DeepEP *before* reinstalling it
RUN curl -L https://github.com/fzyzcjy/DeepEP/commit/814e508537c6ffc775d59f6f1b9ba43f3a65968c.patch -o /root/temp.patch && (cd /sgl-workspace/DeepEP && (patch -p1 < /root/temp.patch))

# temporary hack
RUN rm -rf /sgl-workspace/nvshmem && \
    unset NVSHMEM_DIR && \
    (cd /sgl-workspace/DeepEP && TORCH_CUDA_ARCH_LIST="9.0;10.0;10.3" pip install --no-build-isolation --verbose .)

# temporary hack
RUN SGL_KERNEL_VERSION=0.3.17 && python3 -m pip install https://github.com/sgl-project/whl/releases/download/v${SGL_KERNEL_VERSION}/sgl_kernel-${SGL_KERNEL_VERSION}+cu130-cp310-abi3-manylinux2014_$(uname -m).whl --force-reinstall --no-deps

# temporary hack
RUN curl -L https://github.com/NVIDIA/Megatron-LM/commit/d8c6aa4c0b5d4c15ec1196802bce292d4580ed4a.patch -o /root/temp.patch && (cd /root/Megatron-LM && (patch -p1 < /root/temp.patch))
